# Pipeline Avan√ßado de An√°lise de Sess√µes de Usu√°rios com PySpark
### Da "Sessionization" √† Cria√ß√£o de uma Vis√£o 360¬∫ do Cliente

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange.svg)
![Analysis](https://img.shields.io/badge/Analysis-Behavioral-blueviolet.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

Este reposit√≥rio apresenta um pipeline de dados de ponta a ponta, constru√≠do em PySpark, que transforma logs de eventos brutos de e-commerce em datasets anal√≠ticos de alto valor. O projeto vai muito al√©m de uma simples "sessioniza√ß√£o", demonstrando como construir uma base s√≥lida para an√°lises comportamentais, segmenta√ß√£o de clientes e iniciativas de ci√™ncia de dados.

Utilizamos um grande dataset open-source de eventos de uma loja de cosm√©ticos para simular um cen√°rio de produ√ß√£o realista e de larga escala.

## üéØ Vis√£o Geral do Projeto

Logs de eventos brutos, por si s√≥, s√£o como um rio de informa√ß√µes desestruturadas. Para extrair valor, precisamos agrupar as a√ß√µes dos usu√°rios em "sess√µes" coerentes e, a partir delas, entender seus padr√µes de comportamento.

Este pipeline executa uma jornada anal√≠tica completa, transformando dados brutos em dois outputs principais:
1.  **Tabela de Sess√µes:** Cada linha representa uma sess√£o de usu√°rio √∫nica, enriquecida com m√©tricas e features de comportamento (dura√ß√£o, contagem de eventos, an√°lise de funil, etc.).
2.  **Tabela de Usu√°rios (Vis√£o 360¬∫):** Cada linha representa um usu√°rio √∫nico, com m√©tricas agregadas que descrevem todo o seu ciclo de vida (total de sess√µes, tempo m√©dio entre visitas, total de compras, etc.).

## üó∫Ô∏è A Jornada Anal√≠tica: De Eventos a Insights

O pipeline √© estruturado em etapas l√≥gicas que progressivamente enriquecem os dados.

### Etapa 1: Limpeza e Prepara√ß√£o dos Dados
A base de tudo √© ter dados limpos. Esta etapa carrega os dados brutos do arquivo CSV, aplica um schema rigoroso para garantir a consist√™ncia dos tipos de dados, converte as strings de tempo para o formato `Timestamp` do Spark e remove registros com informa√ß√µes essenciais ausentes.

### Etapa 2: A "Sessionization" (O Cora√ß√£o da L√≥gica)
Esta √© a etapa central, onde os eventos s√£o agrupados em sess√µes.

* **Conceito:** Uma sess√£o √© uma sequ√™ncia de eventos de um usu√°rio, interrompida por um per√≠odo de inatividade (ex: 30 minutos).
* **T√©cnica:** Utilizamos **Window Functions** do PySpark para realizar essa tarefa de forma eficiente e escal√°vel.
    1.  `Window.partitionBy("user_id").orderBy("event_timestamp")`: Criamos uma "janela" que nos permite olhar para os eventos de cada usu√°rio de forma sequencial e ordenada no tempo.
    2.  `lag("event_timestamp", 1)`: Dentro dessa janela, usamos a fun√ß√£o `lag` para acessar o timestamp do evento *anterior*.
    3.  `when(diff > timeout, 1)`: Calculamos a diferen√ßa de tempo. Se for maior que o nosso limite de inatividade (ou se for o primeiro evento do usu√°rio), marcamos essa linha como o in√≠cio de uma nova sess√£o.
    4.  `sum(...) over(window)`: Usamos uma soma cumulativa sobre essa marca√ß√£o. O resultado √© um ID de grupo que incrementa a cada nova sess√£o, nos permitindo atribuir um `session_id` √∫nico.

### Etapa 3: Engenharia de Features a N√≠vel de Sess√£o
Com as sess√µes definidas, agregamos os dados para descrever o que aconteceu dentro de cada uma.

* **M√©tricas B√°sicas:** Dura√ß√£o da sess√£o, n√∫mero de eventos, produtos distintos visualizados.
* **Features Temporais:** Hora do dia e dia da semana do in√≠cio da sess√£o, para entender padr√µes de acesso.
* **An√°lise de Funil de Convers√£o:** Analisamos os tipos de evento (`view`, `cart`, `purchase`) dentro de cada sess√£o para determinar o est√°gio mais profundo que o usu√°rio alcan√ßou (`view_only`, `added_to_cart`, ou `completed_purchase`).

### Etapa 4: Agrega√ß√£o para a Vis√£o 360¬∫ do Usu√°rio
A etapa final eleva a an√°lise do n√≠vel de sess√£o para o n√≠vel de usu√°rio, criando um perfil completo do comportamento de cada cliente ao longo do tempo.

* **M√©tricas de Lifetime:** Total de sess√µes, total de eventos, n√∫mero de sess√µes com compra.
* **M√©tricas Comportamentais:** Dura√ß√£o m√©dia das sess√µes.
* **An√°lise de Recorr√™ncia:** Usando a fun√ß√£o `lag` novamente (desta vez sobre as sess√µes), calculamos o **tempo m√©dio entre as visitas** de um usu√°rio, um indicador poderoso de engajamento.

## üèóÔ∏è Arquitetura e Boas Pr√°ticas

* **C√≥digo Modular e Orientado a Objetos:** Toda a l√≥gica est√° encapsulada na classe `AdvancedRetailAnalyticsPipeline`, com m√©todos privados para cada etapa do processo, tornando o c√≥digo limpo e de f√°cil manuten√ß√£o.
* **Configura√ß√£o Centralizada:** Um arquivo `config.yaml` controla todos os par√¢metros e caminhos, permitindo f√°cil ajuste sem alterar o c√≥digo.
* **Otimiza√ß√£o de Performance:** O pipeline utiliza `.cache()` para armazenar em mem√≥ria o DataFrame intermedi√°rio de eventos "sessionizados". Como este DataFrame √© usado como base para m√∫ltiplas agrega√ß√µes, o cache evita rec√°lculos e acelera significativamente o processo.

## üöÄ Como Executar o Projeto

Siga os passos abaixo para configurar e rodar o pipeline.

### 1. Pr√©-requisitos
* Python 3.9 ou superior.
* Java Development Kit (JDK) 8 ou 11.
* Instale as depend√™ncias Python:
    ```bash
    pip install pyspark pyyaml
    ```

### 2. Download do Dataset
Este projeto utiliza um dataset p√∫blico do Kaggle.
1.  V√° para a p√°gina do dataset: [E-commerce behavior data from multi category store](https://www.kaggle.com/datasets/mkechin/ecommerce-behavior-data-from-multi-category-store).
2.  Baixe o arquivo `2019-Oct.csv`.
3.  Crie a seguinte estrutura de diret√≥rios em seu projeto: `data/source/`.
4.  Coloque o arquivo `2019-Oct.csv` dentro da pasta `data/source/`.

### 3. Crie o Arquivo de Configura√ß√£o
Crie um arquivo chamado **`config.yaml`** na raiz do seu projeto com o seguinte conte√∫do:
```yaml
pipeline_name: "AdvancedRetailUserAnalysis"
log_level: "INFO"

spark:
  app_name: "AdvancedRetailAnalyticsPipeline"
  master: "local[*]"

session_timeout_seconds: 1800 # 30 minutos

data:
  source_path: "data/source/2019-Oct.csv"
  output_path:
    sessionized_events: "data/intermediate/sessionized_events.parquet"
    session_features: "data/analytical/session_features.parquet"
    user_summary: "data/analytical/user_summary.parquet"
```

4. Execu√ß√£o

Com os arquivos no lugar, execute o script Python principal:

```Bash
python advanced_retail_analytics_pipeline.py
```


üìä Estrutura dos Dados de Sa√≠da
O pipeline gera dois datasets anal√≠ticos principais no formato Parquet:

session_features.parquet: Cont√©m uma linha por sess√£o, com colunas como:

`session_id`, `user_id`, `session_start_time`, `session_end_time`, `total_events`, `session_duration_seconds`, `funnel_stage`, etc.

user_summary.parquet: Cont√©m uma linha por usu√°rio, com colunas como:

`user_id`, `total_sessions`, `total_events_lifetime`, `total_purchase_sessions`, `avg_session_duration_seconds`, `avg_time_between_sessions_hours`, etc.

Estes datasets s√£o a base para in√∫meras aplica√ß√µes, desde a cria√ß√£o de dashboards em ferramentas de BI at√© a segmenta√ß√£o de clientes e a modelagem de propens√£o √† compra em projetos de ci√™ncia de dados.

